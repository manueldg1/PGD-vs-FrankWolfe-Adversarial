
## 2. Fooling Neural Networks with Optimization: Frank-Wolfe and PGD Methods in Machine Learning (MNIST & ImageNet Study)
**Objective:** Investigate how optimization techniques can craft adversarial examples that mislead deep neural networks with imperceptible input changes.  
**Key Points:**  
- Implemented Frank-Wolfe variants (standard, momentum, away-step, pairwise) and PGD / MI-FGSM.  
- Experiments under L2 and Lâˆž norm constraints on MNIST and ImageNet datasets.  
- Frank-Wolfe with momentum achieves high attack success rates with lower distortion and efficient convergence.  
**Report:** `Fooling_Neural_Networks_Optimization.pdf`  

---

